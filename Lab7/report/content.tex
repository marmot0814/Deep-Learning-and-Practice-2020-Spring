\title{[Lab7] Temporal Difference Learning}
\author{0616014 æ¥Šæ”¿é“}
\maketitle
\thispagestyle{fancy}
\section{Report}
\subsection{A plot shows episode scores of at least 100,000 training episodes.}
\begin{figure}[!ht]
    \begin{center}
        \includegraphics[width=10cm]{score_plt.png}
        \caption{Performance}
    \end{center}
\end{figure}
\subsection{Describe your implementation in detail.}
\begin{lstlisting}
\subsubsection{pattern::estimate}
virtual float estimate(const board& b) const {
    // TODO
    float ret_value = 0;
    for (int i = 0 ; i < iso_last ; i++) {
        int idx = indexof(isomorphic[i], b);
        ret_value += operator[](idx);
    }
    return ret_value;
}
\end{lstlisting}
\paragraph{}
è¨ˆç®—ä¸€å€‹patternåœ¨ä¸€å€‹boardä¸Šçš„æ•¸å€¼ç¸½å’Œ, è¼¸å…¥æœƒçµ¦å®šä¸€å€‹board, è¦åŠ ä¸Šé€™å€‹patternæ‰€æœ‰åŒæ§‹æ–¹å‘çš„æ‰€æœ‰æ•¸å€¼ï¼Œä¸¦å›å‚³å›å»ã€‚
\subsubsection{pattern::update}
\begin{lstlisting}
virtual float update(const board& b, float u) {
    // TODO
    float u_mean = u / iso_last, ret_value = 0;
    for (int i = 0 ; i < iso_last ; i++) {
        int idx = indexof(isomorphic[i], b);
        operator[](idx) += u_mean;
        ret_value += operator[](idx);
    }
    return ret_value;
}
\end{lstlisting}
\paragraph{}
æ›´æ–°ä¸€å€‹patternçš„è¡¨çš„æ•¸å€¼, è¦æ‰¾åˆ°æ‰€æœ‰å’Œä»–åŒæ§‹çš„é‚£äº›æ ¼å­, ä¸¦ä¸”åŠ ä¸Šè¦æ›´æ–°çš„æ•¸å€¼çš„å¹³å‡ã€‚
\subsubsection{pattern::indexOf}
\begin{lstlisting}
size_t indexof(const std::vector<int>& patt, const board& b) const {
    // TODO
    int ret = 0;
    for (int i = 0 ; i < (int)patt.size() ; i++)
        ret |= (b.at(patt[i]) << (4 * i));
    return ret;
}
\end{lstlisting}
\paragraph{}
çµ¦å®šæŸä¸€å€‹patternçš„ç·¨è™Ÿé‚„æœ‰ä¸€å€‹board, è¦æ ¹æ“špatternçš„ç·¨è™Ÿå»boardä¸­æ’ˆå‡ºæ•¸å€¼ä¸¦ä¸”æ‹¼èµ·ä¾†, å°±æœƒå¾—åˆ°indexã€‚
\subsubsection{learning::select\_best\_move}
\begin{lstlisting}
state select_best_move(const board& b) const {
    state after[4] = { 0, 1, 2, 3 }; // up, right, down, left
    state* best = after;
    for (state* move = after; move != after + 4; move++) {
        if (move->assign(b)) {
            // TODO
            move->set_value(move->reward() + estimate(move->after_state()));
            if (move->value() > best->value())
                best = move;
        } else {
            move->set_value(-std::numeric_limits<float>::max());
        }
        debug << "test " << *move;
    }
    return *best;
}
\end{lstlisting}
\paragraph{}
æ ¹æ“šTD learningçš„æ›´æ–°å…¬å¼, é€™å€‹moveçš„valueæ‡‰è©²è¦æ˜¯ä»–é€™æ­¥å¾—åˆ°çš„rewardåŠ ä¸Šä¸‹ä¸€å€‹state(æœªåŠ å…¥æ–°çš„æ–¹å¡Š, after\_state)çš„ä¼°è¨ˆå€¼ã€‚
\subsubsection{learning::update\_epsisode}
\begin{lstlisting}
void update_episode(std::vector<state>& path, float alpha = 0.1) const {
    // TODO
    float sum = 0;
    path.pop_back();
    while (path.size()) {
        state& move = path.back();
        float loss = sum - (move.value() - move.reward());
        sum = move.reward() + update(move.after_state(), alpha * loss);
        path.pop_back();
    }
}
\end{lstlisting}
\paragraph{}
è·‘å®Œä¸€å€‹episode, è¦æ ¹æ“šé€™å€‹episodeä¸­çš„æ±ºç­–è·¯å¾‘ä¾†æ›´æ–°Q table, å…¶ä¸­alphaå°±æ˜¯éæ¸›çš„åƒæ•¸ã€‚
\subsection{Describe the implementation and the usage of ğ‘›-tuple network.}
\paragraph{}
n-tupleå­˜åœ¨çš„ç†ç”±ç‚ºåƒæ˜¯2048é€™ç¨®æ‰€æœ‰å¯èƒ½çš„æ•¸é‡å¤ªé¾å¤§, ç„¡æ³•æ¯å€‹stateéƒ½é–‹ä¸€æ ¼è¡¨æ ¼ä¾†å­˜, å› æ­¤æŠŠæ•´å€‹ç‰ˆé¢åˆ‡æˆå¹¾å€‹ç¾¤çµ„ç•¶æˆä¸€å€‹pattern, å°æ–¼ä¸€å€‹ç‰ˆé¢çš„ä¼°è¨ˆå€¼å°±æ˜¯é€™äº›patternæ‰€æœ‰åŒæ§‹çš„çµ„åˆçš„ç´¯ç©å€¼ã€‚å¯¦ä½œä¸Šæœƒå°‡ç‰ˆé¢ä¸Šæ¯å€‹æ ¼å­éƒ½ç·¨ä¸€å€‹16é€²åˆ¶çš„æ•¸å­—, è€Œä¸€å€‹patternçš„çµ„æˆå³ç‚ºå¹¾å€‹16é€²åˆ¶çš„æ•¸å­—æ‰€çµ„æˆçš„é›†åˆ, ä¸¦äº‹å…ˆé è™•ç†åŒæ§‹ã€‚
\subsection{Explain the TD-backup diagram of V(state).}
\paragraph{}
æµç¨‹ç‚ºæœ‰ä¸€å€‹state, é¸æ“‡äº†ä¸€å€‹action, ä»–æœƒåˆ°ä¸€å€‹after-state, ä¸¦ä¸”envéš¨æ©Ÿæ”¾ä¸€å€‹æ–¹å¡Šåˆ°ä¸€å€‹ç©ºä½åˆ°é”ä¸‹ä¸€å€‹state, V(state)çš„æ›´æ–°å°±æ˜¯åŠ ä¸Šé€™è£¡çš„TD errorã€‚
\subsection{Explain the action selection of V(state) in a diagram.}
\paragraph{}
actionçš„é¸æ“‡ç‚ºé¸æ“‡ä»–èªç‚ºæœ€å¥½çš„ç‹€æ…‹, è¡¡é‡ä¸€å€‹ç‹€æ…‹çš„å¥½å£ä½¿ç”¨n-tuple networkã€‚ä½¿ç”¨çš„æº–å‰‡ç‚ºä»¥ä¸‹æ–¹ç¨‹å¼ã€‚
$$\pi(s) = argmax_a(R_{t + 1} + E[V(S_{t + 1}|S_t = s, A_t = a])$$
\subsection{Explain the TD-backup diagram of V(after-state).}
\paragraph{}
æµç¨‹ç‚ºæœ‰ä¸€å€‹after-state, envéš¨æ©Ÿæ”¾ä¸€å€‹æ–¹å¡Šåˆ°ä¸€å€‹ç©ºä½åˆ°é”ä¸‹ä¸€å€‹state, æ¥ä¸‹ä¾†æˆ‘é¸æ“‡äº†ä¸€å€‹action, ä»–æœƒåˆ°å¦ä¸€å€‹after-state, V(after-state)çš„æ›´æ–°å°±æ˜¯åŠ ä¸Šé€™è£¡çš„TD errorã€‚
\subsection{Explain the action selection of V(after-state) in a diagram.}
\paragraph{}
actionçš„é¸æ“‡ç‚ºé¸æ“‡ä»–èªç‚ºæœ€å¥½çš„ç‹€æ…‹, è¡¡é‡ä¸€å€‹ç‹€æ…‹çš„å¥½å£ä½¿ç”¨n-tuple networkã€‚ä½¿ç”¨çš„æº–å‰‡ç‚ºä»¥ä¸‹æ–¹ç¨‹å¼ã€‚
$$\pi(s) = argmax_a(V^{af}(S_t^{af}))$$
\subsection{Explain the mechanism of temporal difference learning.}
\paragraph{}
TD learningçš„æ©Ÿåˆ¶ç‚ºå°æ–¼æ¯å€‹actionéƒ½æœƒåšä¸€æ¬¡Qå€¼åƒæ•¸æ›´æ–°, ä¸éœ€è¦æœå°‹ä¸€æ®µæ·±åº¦æ‰æœ‰è¾¦æ³•æ›´æ–°, æ›´æ–°çš„ä¾æ“šç‚ºç¾åœ¨çš„state Qå€¼, ä¸‹ä¸€å€‹stateçš„Qå€¼å·²ç¶“èµ°é€™æ­¥çš„rewardã€‚
\subsection{Explain whether the TD-update perform bootstrapping.}
\paragraph{}
Bootstrappingç‚ºé‚Šæ›´æ–°é‚Šä¼°è¨ˆ, TD learningæ¯æ¬¡çš„æ›´æ–°çš„ä¾æ“šæ˜¯ä¸‹ä¸€å€‹stateçš„ä¼°è¨ˆå€¼, å› æ­¤TD learningæœ‰bootstrappingã€‚
\subsection{Explain whether your training is on-policy or off-policy.}
\paragraph{}
é€™æ¬¡çš„trainingæ–¹å¼ç‚ºon-policy, æ¯æ¬¡æ›´æ–°æ‰€ä½¿ç”¨åˆ°çš„dataéƒ½æ˜¯ç”±è©²agentç”¢ç”Ÿçš„ã€‚
\subsection{Other discussions or improvements.}
\paragraph{}
åœ¨actioné¸æ“‡ä¸Šå¯ä»¥å¤šèµ°å¹¾æ­¥, é›–ç„¶æœƒè®“è¨“ç·´é€Ÿåº¦è®Šæ…¢, ä½†æ˜¯å¯ä»¥è®“agentè€ƒæ…®çš„æ›´å®Œæ•´, å¯ä»¥è®“performanceæœ‰æ©Ÿæœƒè®Šé«˜ã€‚
\newpage
\section{Performance}
\subsection{The 2048-tile win rate in 1000 games.}
\begin{figure}[!ht]
    \begin{center}
        \includegraphics[width=15cm]{result.png}
        \caption{Performance}
    \end{center}
\end{figure}
